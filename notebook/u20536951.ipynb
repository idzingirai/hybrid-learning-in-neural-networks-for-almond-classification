{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1><center>Dependencies Installation</center></h1>",
   "id": "f5a8404a3b5a997e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%pip install pandas==2.2.2\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install seaborn==0.13.2\n",
    "%pip install scikit-learn==1.5.2\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1><center>Import all needed libraries</center></h1>",
   "id": "321a828b6903da95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ],
   "id": "bb33ccf81b923134",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1><center>Setup agnostic device</center></h1",
   "id": "2eca646d2c12271"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device_name = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "device = torch.device(device_name)\n",
    "print(f\"Using {device_name} device\")"
   ],
   "id": "31f9569b99618ac3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1><center>Data Preparation</center></h1>",
   "id": "510f85a410dea4b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Read in dataset</h2>",
   "id": "5f08a1c688d0bce3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ROOT_DIR: str = Path().resolve().parent\n",
    "DATA_DIR: str = os.path.join(ROOT_DIR, 'data')\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(f'{DATA_DIR}/almond.csv').drop(columns=[\"Unnamed: 0\"])\n",
    "df"
   ],
   "id": "fcfe77124b7b87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "50087a3ad5c0f842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.tail()",
   "id": "a52ceef92b88898d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.columns",
   "id": "b7780c20454548",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Sanity-check on the dataset</h2>",
   "id": "dd20943095c6ce8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3><li>Find and drop duplicates across rows</li></h3>",
   "id": "2e93bcf874c434c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.duplicated().sum()",
   "id": "17842f071d4302c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.drop_duplicates(inplace=True)",
   "id": "f37bc254c3c90ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.duplicated().sum()",
   "id": "c272b57926dda62e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3><li>Check the shape of the dataset and see if there are any null values in columns</li></h3>",
   "id": "ed0faa9eeb0b5160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "b4c565b5f31cd401",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "b51e788e3b4e39d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.isna().sum()",
   "id": "cf773ac83598156a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(df.isnull().sum() / df.shape[0]) * 100",
   "id": "35ab9eda816bd5b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3><li>Visualize the distribution of the data</li></h3>",
   "id": "ace616938312098f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_columns = df.columns\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, column in enumerate(numeric_columns, 1):\n",
    "    if column == 'Type':\n",
    "        continue\n",
    "    plt.subplot(len(numeric_columns) // 3 + 1, 3, i)\n",
    "    sns.histplot(df[column], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "81143a374f7230d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3><li>Log transform the columns with outliers</li></h3>",
   "id": "2715fe9272b42b15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for column_name in numeric_columns:\n",
    "    if column_name != 'Type' and not (-0.5 < df[column_name].skew() < 0.5):\n",
    "        print(column_name)\n",
    "        df[column_name] = np.log(df[column_name])"
   ],
   "id": "b11d8c5aa53742ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_columns = df.columns\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, column in enumerate(numeric_columns, 1):\n",
    "    if column == 'Type':\n",
    "        continue\n",
    "    plt.subplot(len(numeric_columns) // 3 + 1, 3, i)\n",
    "    sns.histplot(df[column], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7c767c3a77dfc7ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3><li>Fill in the missing null values</li></h3>",
   "id": "4104a57aae7c3753"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_with_missing_values: List[str] = df.columns[df.isnull().any()].tolist()\n",
    "knn_imputer: KNNImputer = KNNImputer(n_neighbors=5)\n",
    "df[columns_with_missing_values] = knn_imputer.fit_transform(df[columns_with_missing_values])\n",
    "df.isnull().sum()"
   ],
   "id": "bba23bcee61d73cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3><li>Find garbage values in the target variable 'Type'</li></h3>",
   "id": "a120f44271ff3253"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "column_name: str = 'Type'\n",
    "print('****' * 5 + ' START ' + \"****\" * 5)\n",
    "print(df[column_name].value_counts())\n",
    "print('****' * 5 + ' END ' + \"****\" * 6)"
   ],
   "id": "80a148f40c4a9459",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "df[column_name].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title(f'Value Counts for {column_name}')\n",
    "plt.ylabel('x')\n",
    "plt.show()"
   ],
   "id": "5e512a5a3fd378b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3><li>Encode the Target Variable</li></h3>",
   "id": "a4bd842e929eb57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_encoder: LabelEncoder = LabelEncoder()\n",
    "df['Type'] = label_encoder.fit_transform(df['Type'])\n",
    "df.head(10)"
   ],
   "id": "f1af3c53b326a924",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Exploratory Data Analysis (EDA)</h2>",
   "id": "f60bf226c16639eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3><li>Perform statistical analysis on the dataset</li></h3>",
   "id": "59275182d0cb4477"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe().T",
   "id": "1262a9a1f4b5c6b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for i, column in enumerate(numeric_columns, 1):\n",
    "    if column == 'Type':\n",
    "        continue\n",
    "    plt.subplot(len(numeric_columns) // 3 + 1, 3, i)\n",
    "    sns.boxplot(x=df[column])\n",
    "    plt.title(f'Box Plot of {column}')\n",
    "    plt.xlabel(column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "df22799ef01b66e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3><li>Standardize the features data</li></h3>",
   "id": "6b53164084797d9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "columns_to_scale: List[str] = df.columns.tolist()\n",
    "columns_to_scale.remove('Type')\n",
    "\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "df.head()"
   ],
   "id": "2347ca02cd3f7ec7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Turn data into tensors and split the data into train and test datasets</h2>",
   "id": "f6bdcf900f05f66d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = df.drop('Type', axis=1)\n",
    "y = df['Type']\n",
    "\n",
    "num_of_features = X.shape[1]\n",
    "num_of_classes = y.nunique()\n",
    "num_of_features, num_of_classes"
   ],
   "id": "c9debd1c1ef48867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_tensor = torch.tensor(X.values, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)"
   ],
   "id": "dc16521d48979aa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1><center>Implementation</center></h1>",
   "id": "820ddc17f64d05fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Setup NeuralNetwork</h2>\n",
   "id": "74562a07f8a41bbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AlmondClassificationNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            seed: int,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            hidden_layers=None\n",
    "    ):\n",
    "        super(AlmondClassificationNN, self).__init__()\n",
    "        self.seed = seed\n",
    "        self._seed_model()\n",
    "\n",
    "        if hidden_layers is None:\n",
    "            hidden_layers = [(10, 10)]\n",
    "\n",
    "        self.input = nn.Linear(in_features, hidden_layers[0][0])\n",
    "        self.input_activation = nn.ReLU()\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.num_of_hidden_layers = len(hidden_layers)\n",
    "        for index in range(0, self.num_of_hidden_layers):\n",
    "            in_neurons, out_neurons = hidden_layers[index]\n",
    "            self.hidden_layers.append(nn.Linear(in_neurons, out_neurons))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "\n",
    "        self.output = nn.Linear(hidden_layers[-1][1], out_features)\n",
    "        self.output_activation = F.softmax\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def _seed_model(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed(self.seed)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(self.seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.input_activation(self.input(features))\n",
    "\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            features = hidden_layer(features)\n",
    "\n",
    "        return self.output_activation(self.output(features), dim=-1)"
   ],
   "id": "4a6c1c2b9450e82e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Setup neural network and training algorithms / optimizers</h2>",
   "id": "3aee7c39b0e2b4bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T18:22:39.731491Z",
     "start_time": "2024-10-01T18:22:39.721520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "\n",
    "def grid_search(\n",
    "        seed: int,\n",
    "        num_of_splits: int,\n",
    "        param_grid: dict,\n",
    "        num_of_epoches: int, \n",
    "        optimizer_name: str, \n",
    "        optimizer_momentum: float\n",
    "):\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    for param_combination in param_combinations:\n",
    "        learning_rate = param_combination['learning_rate']\n",
    "        hidden_layers: List[Tuple[int, int]] = param_combination['hidden_layers']\n",
    "\n",
    "        kf: KFold = KFold(n_splits=num_of_splits, shuffle=True, random_state=seed)\n",
    "        fold_accuracies = []\n",
    "        fold_losses = []\n",
    "\n",
    "        for fold, (train_index, validation_index) in enumerate(kf.split(dataset)):\n",
    "            train_subset = Subset(dataset, train_index)\n",
    "            validation_subset = Subset(dataset, validation_index)\n",
    "\n",
    "            train_loader = DataLoader(train_subset, batch_size=12, shuffle=True)\n",
    "            validation_loader = DataLoader(validation_subset, batch_size=12)\n",
    "            \n",
    "            neuralnetwork = AlmondClassificationNN(\n",
    "                seed=seed,\n",
    "                in_features=num_of_features,\n",
    "                out_features=num_of_classes,\n",
    "                hidden_layers=hidden_layers,\n",
    "            )\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            optimizers_dict = {\n",
    "                'sgd': optim.SGD(neuralnetwork.parameters(), lr=learning_rate, momentum=optimizer_momentum),\n",
    "                'adam': optim.Adam(neuralnetwork.parameters(), lr=learning_rate),\n",
    "                'rmsprop': optim.RMSprop(neuralnetwork.parameters(), lr=learning_rate),\n",
    "                'rprop': optim.Rprop(neuralnetwork.parameters(), lr=learning_rate)\n",
    "            }\n",
    "            optimizer = optimizers_dict.get(optimizer_name)\n",
    "\n",
    "            for epoch in range(num_of_epoches):\n",
    "                neuralnetwork.train()\n",
    "                running_loss = 0.0\n",
    "\n",
    "                for X_train_batch, y_train_batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_prediction = neuralnetwork(X_train_batch)\n",
    "\n",
    "                    loss = criterion(y_batch_prediction, y_train_batch.long())\n",
    "                    loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                print(f'Epoch [{epoch + 1}/{num_of_epoches}], Training Loss: {running_loss / len(train_loader):.4f}')\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                neuralnetwork.eval()\n",
    "                \n",
    "                val_preds = []\n",
    "                val_true = []\n",
    "                running_val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for X_validation_batch, y_validation_batch in validation_loader:\n",
    "                        y_validation_prediction = neuralnetwork(X_validation_batch)\n",
    "                        loss = criterion(y_validation_prediction, y_validation_batch.long())\n",
    "                        running_val_loss += loss.item()\n",
    "    \n",
    "                        _, predicted = torch.max(y_validation_prediction, 1)\n",
    "                        val_preds.extend(predicted.numpy())\n",
    "                        val_true.extend(y_validation_batch.numpy())\n",
    "\n",
    "                    avg_val_loss = running_val_loss / len(validation_loader)\n",
    "                    accuracy = accuracy_score(val_true, val_preds)\n",
    "                    fold_accuracies.append(accuracy)\n",
    "                    fold_losses.append(avg_val_loss)\n",
    "            \n",
    "            print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "        avg_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "        avg_loss = sum(fold_losses) / len(fold_losses)\n",
    "        print(f'  Average Loss: {avg_loss:.4f}, Average Accuracy: {avg_accuracy:.4f} for parameters {param_combination}')\n"
   ],
   "id": "66630416b9aeaec2",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T18:25:43.885233Z",
     "start_time": "2024-10-01T18:22:40.352653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "grid_search(\n",
    "    seed=999,\n",
    "    num_of_splits=2,\n",
    "    param_grid={\n",
    "        'learning_rate': [0.01, 0.003, 0.005, 0.001],\n",
    "        'hidden_layers': [[(512, 256), (256, 128), (128, 64), (64,32), (32, 16)]]\n",
    "    },\n",
    "    num_of_epoches=100,\n",
    "    optimizer_name='sgd',\n",
    "    optimizer_momentum=0.5\n",
    ")"
   ],
   "id": "418d26db245e6160",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 0.9933\n",
      "Epoch [2/100], Training Loss: 0.9296\n",
      "Epoch [3/100], Training Loss: 0.9030\n",
      "Epoch [4/100], Training Loss: 0.8841\n",
      "Epoch [5/100], Training Loss: 0.8656\n",
      "Epoch [6/100], Training Loss: 0.8477\n",
      "Epoch [7/100], Training Loss: 0.8388\n",
      "Epoch [8/100], Training Loss: 0.8296\n",
      "Epoch [9/100], Training Loss: 0.8167\n",
      "Epoch [10/100], Training Loss: 0.8083\n",
      "Epoch [11/100], Training Loss: 0.8125\n",
      "Epoch [12/100], Training Loss: 0.8062\n",
      "Epoch [13/100], Training Loss: 0.7890\n",
      "Epoch [14/100], Training Loss: 0.7905\n",
      "Epoch [15/100], Training Loss: 0.7964\n",
      "Epoch [16/100], Training Loss: 0.7854\n",
      "Epoch [17/100], Training Loss: 0.7799\n",
      "Epoch [18/100], Training Loss: 0.7769\n",
      "Epoch [19/100], Training Loss: 0.7607\n",
      "Epoch [20/100], Training Loss: 0.7637\n",
      "Epoch [21/100], Training Loss: 0.7731\n",
      "Epoch [22/100], Training Loss: 0.7633\n",
      "Epoch [23/100], Training Loss: 0.7728\n",
      "Epoch [24/100], Training Loss: 0.7522\n",
      "Epoch [25/100], Training Loss: 0.7481\n",
      "Epoch [26/100], Training Loss: 0.7449\n",
      "Epoch [27/100], Training Loss: 0.7450\n",
      "Epoch [28/100], Training Loss: 0.7511\n",
      "Epoch [29/100], Training Loss: 0.7442\n",
      "Epoch [30/100], Training Loss: 0.7498\n",
      "Epoch [31/100], Training Loss: 0.7281\n",
      "Epoch [32/100], Training Loss: 0.7334\n",
      "Epoch [33/100], Training Loss: 0.7361\n",
      "Epoch [34/100], Training Loss: 0.7307\n",
      "Epoch [35/100], Training Loss: 0.7379\n",
      "Epoch [36/100], Training Loss: 0.7371\n",
      "Epoch [37/100], Training Loss: 0.7260\n",
      "Epoch [38/100], Training Loss: 0.7384\n",
      "Epoch [39/100], Training Loss: 0.7319\n",
      "Epoch [40/100], Training Loss: 0.7268\n",
      "Epoch [41/100], Training Loss: 0.7187\n",
      "Epoch [42/100], Training Loss: 0.7243\n",
      "Epoch [43/100], Training Loss: 0.7231\n",
      "Epoch [44/100], Training Loss: 0.7177\n",
      "Epoch [45/100], Training Loss: 0.7207\n",
      "Epoch [46/100], Training Loss: 0.7201\n",
      "Epoch [47/100], Training Loss: 0.7210\n",
      "Epoch [48/100], Training Loss: 0.7192\n",
      "Epoch [49/100], Training Loss: 0.7136\n",
      "Epoch [50/100], Training Loss: 0.7155\n",
      "Epoch [51/100], Training Loss: 0.7103\n",
      "Epoch [52/100], Training Loss: 0.7138\n",
      "Epoch [53/100], Training Loss: 0.7030\n",
      "Epoch [54/100], Training Loss: 0.7072\n",
      "Epoch [55/100], Training Loss: 0.7046\n",
      "Epoch [56/100], Training Loss: 0.7069\n",
      "Epoch [57/100], Training Loss: 0.7007\n",
      "Epoch [58/100], Training Loss: 0.7025\n",
      "Epoch [59/100], Training Loss: 0.7028\n",
      "Epoch [60/100], Training Loss: 0.6884\n",
      "Epoch [61/100], Training Loss: 0.6903\n",
      "Epoch [62/100], Training Loss: 0.6922\n",
      "Epoch [63/100], Training Loss: 0.6866\n",
      "Epoch [64/100], Training Loss: 0.7034\n",
      "Epoch [65/100], Training Loss: 0.6970\n",
      "Epoch [66/100], Training Loss: 0.6854\n",
      "Epoch [67/100], Training Loss: 0.6862\n",
      "Epoch [68/100], Training Loss: 0.6923\n",
      "Epoch [69/100], Training Loss: 0.6790\n",
      "Epoch [70/100], Training Loss: 0.6910\n",
      "Epoch [71/100], Training Loss: 0.6949\n",
      "Epoch [72/100], Training Loss: 0.6837\n",
      "Epoch [73/100], Training Loss: 0.6883\n",
      "Epoch [74/100], Training Loss: 0.7021\n",
      "Epoch [75/100], Training Loss: 0.6859\n",
      "Epoch [76/100], Training Loss: 0.6835\n",
      "Epoch [77/100], Training Loss: 0.6744\n",
      "Epoch [78/100], Training Loss: 0.6696\n",
      "Epoch [79/100], Training Loss: 0.6706\n",
      "Epoch [80/100], Training Loss: 0.6782\n",
      "Epoch [81/100], Training Loss: 0.6772\n",
      "Epoch [82/100], Training Loss: 0.6719\n",
      "Epoch [83/100], Training Loss: 0.6734\n",
      "Epoch [84/100], Training Loss: 0.6725\n",
      "Epoch [85/100], Training Loss: 0.6706\n",
      "Epoch [86/100], Training Loss: 0.6738\n",
      "Epoch [87/100], Training Loss: 0.6632\n",
      "Epoch [88/100], Training Loss: 0.6774\n",
      "Epoch [89/100], Training Loss: 0.6652\n",
      "Epoch [90/100], Training Loss: 0.6634\n",
      "Epoch [91/100], Training Loss: 0.6795\n",
      "Epoch [92/100], Training Loss: 0.6613\n",
      "Epoch [93/100], Training Loss: 0.6762\n",
      "Epoch [94/100], Training Loss: 0.6683\n",
      "Epoch [95/100], Training Loss: 0.6713\n",
      "Epoch [96/100], Training Loss: 0.6656\n",
      "Epoch [97/100], Training Loss: 0.6650\n",
      "Epoch [98/100], Training Loss: 0.6662\n",
      "Epoch [99/100], Training Loss: 0.6538\n",
      "Epoch [100/100], Training Loss: 0.6505\n",
      "Validation Loss: 0.8331, Validation Accuracy: 0.7121\n",
      "Epoch [1/100], Training Loss: 1.0107\n",
      "Epoch [2/100], Training Loss: 0.9408\n",
      "Epoch [3/100], Training Loss: 0.9101\n",
      "Epoch [4/100], Training Loss: 0.8952\n",
      "Epoch [5/100], Training Loss: 0.8845\n",
      "Epoch [6/100], Training Loss: 0.8706\n",
      "Epoch [7/100], Training Loss: 0.8675\n",
      "Epoch [8/100], Training Loss: 0.8590\n",
      "Epoch [9/100], Training Loss: 0.8465\n",
      "Epoch [10/100], Training Loss: 0.8420\n",
      "Epoch [11/100], Training Loss: 0.8422\n",
      "Epoch [12/100], Training Loss: 0.8366\n",
      "Epoch [13/100], Training Loss: 0.8293\n",
      "Epoch [14/100], Training Loss: 0.8206\n",
      "Epoch [15/100], Training Loss: 0.8177\n",
      "Epoch [16/100], Training Loss: 0.8231\n",
      "Epoch [17/100], Training Loss: 0.7992\n",
      "Epoch [18/100], Training Loss: 0.7982\n",
      "Epoch [19/100], Training Loss: 0.7949\n",
      "Epoch [20/100], Training Loss: 0.7970\n",
      "Epoch [21/100], Training Loss: 0.7962\n",
      "Epoch [22/100], Training Loss: 0.7878\n",
      "Epoch [23/100], Training Loss: 0.7898\n",
      "Epoch [24/100], Training Loss: 0.7833\n",
      "Epoch [25/100], Training Loss: 0.7808\n",
      "Epoch [26/100], Training Loss: 0.7734\n",
      "Epoch [27/100], Training Loss: 0.7860\n",
      "Epoch [28/100], Training Loss: 0.7745\n",
      "Epoch [29/100], Training Loss: 0.7917\n",
      "Epoch [30/100], Training Loss: 0.7752\n",
      "Epoch [31/100], Training Loss: 0.7696\n",
      "Epoch [32/100], Training Loss: 0.7621\n",
      "Epoch [33/100], Training Loss: 0.7537\n",
      "Epoch [34/100], Training Loss: 0.7641\n",
      "Epoch [35/100], Training Loss: 0.7670\n",
      "Epoch [36/100], Training Loss: 0.7626\n",
      "Epoch [37/100], Training Loss: 0.7478\n",
      "Epoch [38/100], Training Loss: 0.7568\n",
      "Epoch [39/100], Training Loss: 0.7436\n",
      "Epoch [40/100], Training Loss: 0.7578\n",
      "Epoch [41/100], Training Loss: 0.7489\n",
      "Epoch [42/100], Training Loss: 0.7528\n",
      "Epoch [43/100], Training Loss: 0.7479\n",
      "Epoch [44/100], Training Loss: 0.7486\n",
      "Epoch [45/100], Training Loss: 0.7497\n",
      "Epoch [46/100], Training Loss: 0.7419\n",
      "Epoch [47/100], Training Loss: 0.7532\n",
      "Epoch [48/100], Training Loss: 0.7329\n",
      "Epoch [49/100], Training Loss: 0.7470\n",
      "Epoch [50/100], Training Loss: 0.7424\n",
      "Epoch [51/100], Training Loss: 0.7310\n",
      "Epoch [52/100], Training Loss: 0.7355\n",
      "Epoch [53/100], Training Loss: 0.7254\n",
      "Epoch [54/100], Training Loss: 0.7208\n",
      "Epoch [55/100], Training Loss: 0.7265\n",
      "Epoch [56/100], Training Loss: 0.7216\n",
      "Epoch [57/100], Training Loss: 0.7253\n",
      "Epoch [58/100], Training Loss: 0.7235\n",
      "Epoch [59/100], Training Loss: 0.7238\n",
      "Epoch [60/100], Training Loss: 0.7183\n",
      "Epoch [61/100], Training Loss: 0.7115\n",
      "Epoch [62/100], Training Loss: 0.7180\n",
      "Epoch [63/100], Training Loss: 0.7234\n",
      "Epoch [64/100], Training Loss: 0.7249\n",
      "Epoch [65/100], Training Loss: 0.7238\n",
      "Epoch [66/100], Training Loss: 0.7254\n",
      "Epoch [67/100], Training Loss: 0.7118\n",
      "Epoch [68/100], Training Loss: 0.7158\n",
      "Epoch [69/100], Training Loss: 0.7129\n",
      "Epoch [70/100], Training Loss: 0.7087\n",
      "Epoch [71/100], Training Loss: 0.7337\n",
      "Epoch [72/100], Training Loss: 0.7131\n",
      "Epoch [73/100], Training Loss: 0.7031\n",
      "Epoch [74/100], Training Loss: 0.7036\n",
      "Epoch [75/100], Training Loss: 0.7046\n",
      "Epoch [76/100], Training Loss: 0.7130\n",
      "Epoch [77/100], Training Loss: 0.7146\n",
      "Epoch [78/100], Training Loss: 0.7117\n",
      "Epoch [79/100], Training Loss: 0.7027\n",
      "Epoch [80/100], Training Loss: 0.7101\n",
      "Epoch [81/100], Training Loss: 0.7053\n",
      "Epoch [82/100], Training Loss: 0.7034\n",
      "Epoch [83/100], Training Loss: 0.7014\n",
      "Epoch [84/100], Training Loss: 0.7065\n",
      "Epoch [85/100], Training Loss: 0.7064\n",
      "Epoch [86/100], Training Loss: 0.7095\n",
      "Epoch [87/100], Training Loss: 0.7175\n",
      "Epoch [88/100], Training Loss: 0.7017\n",
      "Epoch [89/100], Training Loss: 0.7034\n",
      "Epoch [90/100], Training Loss: 0.6972\n",
      "Epoch [91/100], Training Loss: 0.7228\n",
      "Epoch [92/100], Training Loss: 0.7169\n",
      "Epoch [93/100], Training Loss: 0.7254\n",
      "Epoch [94/100], Training Loss: 0.6964\n",
      "Epoch [95/100], Training Loss: 0.6997\n",
      "Epoch [96/100], Training Loss: 0.6891\n",
      "Epoch [97/100], Training Loss: 0.6963\n",
      "Epoch [98/100], Training Loss: 0.6968\n",
      "Epoch [99/100], Training Loss: 0.7035\n",
      "Epoch [100/100], Training Loss: 0.7026\n",
      "Validation Loss: 0.8138, Validation Accuracy: 0.7343\n",
      "  Average Loss: 0.8234, Average Accuracy: 0.7232 for parameters {'learning_rate': 0.01, 'hidden_layers': [(512, 256), (256, 128), (128, 64), (64, 32), (32, 16)]}\n",
      "Epoch [1/100], Training Loss: 1.0161\n",
      "Epoch [2/100], Training Loss: 0.9502\n",
      "Epoch [3/100], Training Loss: 0.9226\n",
      "Epoch [4/100], Training Loss: 0.9073\n",
      "Epoch [5/100], Training Loss: 0.8912\n",
      "Epoch [6/100], Training Loss: 0.8748\n",
      "Epoch [7/100], Training Loss: 0.8631\n",
      "Epoch [8/100], Training Loss: 0.8503\n",
      "Epoch [9/100], Training Loss: 0.8395\n",
      "Epoch [10/100], Training Loss: 0.8294\n",
      "Epoch [11/100], Training Loss: 0.8224\n",
      "Epoch [12/100], Training Loss: 0.8174\n",
      "Epoch [13/100], Training Loss: 0.8041\n",
      "Epoch [14/100], Training Loss: 0.8003\n",
      "Epoch [15/100], Training Loss: 0.7937\n",
      "Epoch [16/100], Training Loss: 0.7850\n",
      "Epoch [17/100], Training Loss: 0.7843\n",
      "Epoch [18/100], Training Loss: 0.7796\n",
      "Epoch [19/100], Training Loss: 0.7700\n",
      "Epoch [20/100], Training Loss: 0.7728\n",
      "Epoch [21/100], Training Loss: 0.7652\n",
      "Epoch [22/100], Training Loss: 0.7663\n",
      "Epoch [23/100], Training Loss: 0.7544\n",
      "Epoch [24/100], Training Loss: 0.7555\n",
      "Epoch [25/100], Training Loss: 0.7497\n",
      "Epoch [26/100], Training Loss: 0.7468\n",
      "Epoch [27/100], Training Loss: 0.7440\n",
      "Epoch [28/100], Training Loss: 0.7465\n",
      "Epoch [29/100], Training Loss: 0.7431\n",
      "Epoch [30/100], Training Loss: 0.7386\n",
      "Epoch [31/100], Training Loss: 0.7321\n",
      "Epoch [32/100], Training Loss: 0.7345\n",
      "Epoch [33/100], Training Loss: 0.7283\n",
      "Epoch [34/100], Training Loss: 0.7246\n",
      "Epoch [35/100], Training Loss: 0.7304\n",
      "Epoch [36/100], Training Loss: 0.7207\n",
      "Epoch [37/100], Training Loss: 0.7167\n",
      "Epoch [38/100], Training Loss: 0.7173\n",
      "Epoch [39/100], Training Loss: 0.7172\n",
      "Epoch [40/100], Training Loss: 0.7197\n",
      "Epoch [41/100], Training Loss: 0.7151\n",
      "Epoch [42/100], Training Loss: 0.7176\n",
      "Epoch [43/100], Training Loss: 0.7159\n",
      "Epoch [44/100], Training Loss: 0.7159\n",
      "Epoch [45/100], Training Loss: 0.7072\n",
      "Epoch [46/100], Training Loss: 0.7042\n",
      "Epoch [47/100], Training Loss: 0.7157\n",
      "Epoch [48/100], Training Loss: 0.7065\n",
      "Epoch [49/100], Training Loss: 0.7050\n",
      "Epoch [50/100], Training Loss: 0.7087\n",
      "Epoch [51/100], Training Loss: 0.7070\n",
      "Epoch [52/100], Training Loss: 0.6950\n",
      "Epoch [53/100], Training Loss: 0.7024\n",
      "Epoch [54/100], Training Loss: 0.7025\n",
      "Epoch [55/100], Training Loss: 0.7026\n",
      "Epoch [56/100], Training Loss: 0.6937\n",
      "Epoch [57/100], Training Loss: 0.6865\n",
      "Epoch [58/100], Training Loss: 0.6845\n",
      "Epoch [59/100], Training Loss: 0.6922\n",
      "Epoch [60/100], Training Loss: 0.6901\n",
      "Epoch [61/100], Training Loss: 0.6924\n",
      "Epoch [62/100], Training Loss: 0.6940\n",
      "Epoch [63/100], Training Loss: 0.6847\n",
      "Epoch [64/100], Training Loss: 0.6825\n",
      "Epoch [65/100], Training Loss: 0.6858\n",
      "Epoch [66/100], Training Loss: 0.6938\n",
      "Epoch [67/100], Training Loss: 0.6831\n",
      "Epoch [68/100], Training Loss: 0.6810\n",
      "Epoch [69/100], Training Loss: 0.6761\n",
      "Epoch [70/100], Training Loss: 0.6715\n",
      "Epoch [71/100], Training Loss: 0.6790\n",
      "Epoch [72/100], Training Loss: 0.6773\n",
      "Epoch [73/100], Training Loss: 0.6843\n",
      "Epoch [74/100], Training Loss: 0.6721\n",
      "Epoch [75/100], Training Loss: 0.6777\n",
      "Epoch [76/100], Training Loss: 0.6742\n",
      "Epoch [77/100], Training Loss: 0.6730\n",
      "Epoch [78/100], Training Loss: 0.6698\n",
      "Epoch [79/100], Training Loss: 0.6841\n",
      "Epoch [80/100], Training Loss: 0.6815\n",
      "Epoch [81/100], Training Loss: 0.6731\n",
      "Epoch [82/100], Training Loss: 0.6668\n",
      "Epoch [83/100], Training Loss: 0.6691\n",
      "Epoch [84/100], Training Loss: 0.6736\n",
      "Epoch [85/100], Training Loss: 0.6661\n",
      "Epoch [86/100], Training Loss: 0.6786\n",
      "Epoch [87/100], Training Loss: 0.6638\n",
      "Epoch [88/100], Training Loss: 0.6660\n",
      "Epoch [89/100], Training Loss: 0.6632\n",
      "Epoch [90/100], Training Loss: 0.6708\n",
      "Epoch [91/100], Training Loss: 0.6633\n",
      "Epoch [92/100], Training Loss: 0.6545\n",
      "Epoch [93/100], Training Loss: 0.6634\n",
      "Epoch [94/100], Training Loss: 0.6718\n",
      "Epoch [95/100], Training Loss: 0.6579\n",
      "Epoch [96/100], Training Loss: 0.6638\n",
      "Epoch [97/100], Training Loss: 0.6641\n",
      "Epoch [98/100], Training Loss: 0.6606\n",
      "Epoch [99/100], Training Loss: 0.6619\n",
      "Epoch [100/100], Training Loss: 0.6738\n",
      "Validation Loss: 0.8445, Validation Accuracy: 0.7000\n",
      "Epoch [1/100], Training Loss: 1.0306\n",
      "Epoch [2/100], Training Loss: 0.9654\n",
      "Epoch [3/100], Training Loss: 0.9352\n",
      "Epoch [4/100], Training Loss: 0.9157\n",
      "Epoch [5/100], Training Loss: 0.9014\n",
      "Epoch [6/100], Training Loss: 0.8868\n",
      "Epoch [7/100], Training Loss: 0.8797\n",
      "Epoch [8/100], Training Loss: 0.8712\n",
      "Epoch [9/100], Training Loss: 0.8616\n",
      "Epoch [10/100], Training Loss: 0.8554\n",
      "Epoch [11/100], Training Loss: 0.8476\n",
      "Epoch [12/100], Training Loss: 0.8440\n",
      "Epoch [13/100], Training Loss: 0.8352\n",
      "Epoch [14/100], Training Loss: 0.8305\n",
      "Epoch [15/100], Training Loss: 0.8282\n",
      "Epoch [16/100], Training Loss: 0.8207\n",
      "Epoch [17/100], Training Loss: 0.8176\n",
      "Epoch [18/100], Training Loss: 0.8121\n",
      "Epoch [19/100], Training Loss: 0.8118\n",
      "Epoch [20/100], Training Loss: 0.8037\n",
      "Epoch [21/100], Training Loss: 0.8039\n",
      "Epoch [22/100], Training Loss: 0.7927\n",
      "Epoch [23/100], Training Loss: 0.7947\n",
      "Epoch [24/100], Training Loss: 0.7945\n",
      "Epoch [25/100], Training Loss: 0.7873\n",
      "Epoch [26/100], Training Loss: 0.7858\n",
      "Epoch [27/100], Training Loss: 0.7837\n",
      "Epoch [28/100], Training Loss: 0.7815\n",
      "Epoch [29/100], Training Loss: 0.7725\n",
      "Epoch [30/100], Training Loss: 0.7795\n",
      "Epoch [31/100], Training Loss: 0.7778\n",
      "Epoch [32/100], Training Loss: 0.7763\n",
      "Epoch [33/100], Training Loss: 0.7651\n",
      "Epoch [34/100], Training Loss: 0.7684\n",
      "Epoch [35/100], Training Loss: 0.7631\n",
      "Epoch [36/100], Training Loss: 0.7604\n",
      "Epoch [37/100], Training Loss: 0.7578\n",
      "Epoch [38/100], Training Loss: 0.7593\n",
      "Epoch [39/100], Training Loss: 0.7582\n",
      "Epoch [40/100], Training Loss: 0.7554\n",
      "Epoch [41/100], Training Loss: 0.7556\n",
      "Epoch [42/100], Training Loss: 0.7506\n",
      "Epoch [43/100], Training Loss: 0.7509\n",
      "Epoch [44/100], Training Loss: 0.7481\n",
      "Epoch [45/100], Training Loss: 0.7550\n",
      "Epoch [46/100], Training Loss: 0.7468\n",
      "Epoch [47/100], Training Loss: 0.7426\n",
      "Epoch [48/100], Training Loss: 0.7418\n",
      "Epoch [49/100], Training Loss: 0.7381\n",
      "Epoch [50/100], Training Loss: 0.7352\n",
      "Epoch [51/100], Training Loss: 0.7426\n",
      "Epoch [52/100], Training Loss: 0.7399\n",
      "Epoch [53/100], Training Loss: 0.7495\n",
      "Epoch [54/100], Training Loss: 0.7359\n",
      "Epoch [55/100], Training Loss: 0.7340\n",
      "Epoch [56/100], Training Loss: 0.7292\n",
      "Epoch [57/100], Training Loss: 0.7327\n",
      "Epoch [58/100], Training Loss: 0.7315\n",
      "Epoch [59/100], Training Loss: 0.7351\n",
      "Epoch [60/100], Training Loss: 0.7239\n",
      "Epoch [61/100], Training Loss: 0.7249\n",
      "Epoch [62/100], Training Loss: 0.7118\n",
      "Epoch [63/100], Training Loss: 0.7184\n",
      "Epoch [64/100], Training Loss: 0.7205\n",
      "Epoch [65/100], Training Loss: 0.7222\n",
      "Epoch [66/100], Training Loss: 0.7231\n",
      "Epoch [67/100], Training Loss: 0.7173\n",
      "Epoch [68/100], Training Loss: 0.7146\n",
      "Epoch [69/100], Training Loss: 0.7195\n",
      "Epoch [70/100], Training Loss: 0.7126\n",
      "Epoch [71/100], Training Loss: 0.7231\n",
      "Epoch [72/100], Training Loss: 0.7186\n",
      "Epoch [73/100], Training Loss: 0.7112\n",
      "Epoch [74/100], Training Loss: 0.7133\n",
      "Epoch [75/100], Training Loss: 0.7161\n",
      "Epoch [76/100], Training Loss: 0.7090\n",
      "Epoch [77/100], Training Loss: 0.7019\n",
      "Epoch [78/100], Training Loss: 0.7007\n",
      "Epoch [79/100], Training Loss: 0.7001\n",
      "Epoch [80/100], Training Loss: 0.7138\n",
      "Epoch [81/100], Training Loss: 0.7099\n",
      "Epoch [82/100], Training Loss: 0.7166\n",
      "Epoch [83/100], Training Loss: 0.7060\n",
      "Epoch [84/100], Training Loss: 0.7054\n",
      "Epoch [85/100], Training Loss: 0.6965\n",
      "Epoch [86/100], Training Loss: 0.6977\n",
      "Epoch [87/100], Training Loss: 0.6988\n",
      "Epoch [88/100], Training Loss: 0.6939\n",
      "Epoch [89/100], Training Loss: 0.7170\n",
      "Epoch [90/100], Training Loss: 0.6991\n",
      "Epoch [91/100], Training Loss: 0.6999\n",
      "Epoch [92/100], Training Loss: 0.6935\n",
      "Epoch [93/100], Training Loss: 0.6946\n",
      "Epoch [94/100], Training Loss: 0.6924\n",
      "Epoch [95/100], Training Loss: 0.6976\n",
      "Epoch [96/100], Training Loss: 0.6829\n",
      "Epoch [97/100], Training Loss: 0.6880\n",
      "Epoch [98/100], Training Loss: 0.6926\n",
      "Epoch [99/100], Training Loss: 0.6974\n",
      "Epoch [100/100], Training Loss: 0.6888\n",
      "Validation Loss: 0.8269, Validation Accuracy: 0.7214\n",
      "  Average Loss: 0.8357, Average Accuracy: 0.7107 for parameters {'learning_rate': 0.005, 'hidden_layers': [(512, 256), (256, 128), (128, 64), (64, 32), (32, 16)]}\n",
      "Epoch [1/100], Training Loss: 1.0750\n",
      "Epoch [2/100], Training Loss: 1.0201\n",
      "Epoch [3/100], Training Loss: 0.9923\n",
      "Epoch [4/100], Training Loss: 0.9762\n",
      "Epoch [5/100], Training Loss: 0.9635\n",
      "Epoch [6/100], Training Loss: 0.9527\n",
      "Epoch [7/100], Training Loss: 0.9450\n",
      "Epoch [8/100], Training Loss: 0.9377\n",
      "Epoch [9/100], Training Loss: 0.9322\n",
      "Epoch [10/100], Training Loss: 0.9260\n",
      "Epoch [11/100], Training Loss: 0.9196\n",
      "Epoch [12/100], Training Loss: 0.9162\n",
      "Epoch [13/100], Training Loss: 0.9109\n",
      "Epoch [14/100], Training Loss: 0.9066\n",
      "Epoch [15/100], Training Loss: 0.9030\n",
      "Epoch [16/100], Training Loss: 0.8977\n",
      "Epoch [17/100], Training Loss: 0.8957\n",
      "Epoch [18/100], Training Loss: 0.8913\n",
      "Epoch [19/100], Training Loss: 0.8875\n",
      "Epoch [20/100], Training Loss: 0.8842\n",
      "Epoch [21/100], Training Loss: 0.8800\n",
      "Epoch [22/100], Training Loss: 0.8772\n",
      "Epoch [23/100], Training Loss: 0.8722\n",
      "Epoch [24/100], Training Loss: 0.8706\n",
      "Epoch [25/100], Training Loss: 0.8670\n",
      "Epoch [26/100], Training Loss: 0.8628\n",
      "Epoch [27/100], Training Loss: 0.8604\n",
      "Epoch [28/100], Training Loss: 0.8579\n",
      "Epoch [29/100], Training Loss: 0.8545\n",
      "Epoch [30/100], Training Loss: 0.8506\n",
      "Epoch [31/100], Training Loss: 0.8479\n",
      "Epoch [32/100], Training Loss: 0.8450\n",
      "Epoch [33/100], Training Loss: 0.8422\n",
      "Epoch [34/100], Training Loss: 0.8391\n",
      "Epoch [35/100], Training Loss: 0.8369\n",
      "Epoch [36/100], Training Loss: 0.8334\n",
      "Epoch [37/100], Training Loss: 0.8304\n",
      "Epoch [38/100], Training Loss: 0.8274\n",
      "Epoch [39/100], Training Loss: 0.8241\n",
      "Epoch [40/100], Training Loss: 0.8219\n",
      "Epoch [41/100], Training Loss: 0.8195\n",
      "Epoch [42/100], Training Loss: 0.8166\n",
      "Epoch [43/100], Training Loss: 0.8144\n",
      "Epoch [44/100], Training Loss: 0.8120\n",
      "Epoch [45/100], Training Loss: 0.8079\n",
      "Epoch [46/100], Training Loss: 0.8066\n",
      "Epoch [47/100], Training Loss: 0.8025\n",
      "Epoch [48/100], Training Loss: 0.8021\n",
      "Epoch [49/100], Training Loss: 0.8005\n",
      "Epoch [50/100], Training Loss: 0.7976\n",
      "Epoch [51/100], Training Loss: 0.7952\n",
      "Epoch [52/100], Training Loss: 0.7931\n",
      "Epoch [53/100], Training Loss: 0.7920\n",
      "Epoch [54/100], Training Loss: 0.7885\n",
      "Epoch [55/100], Training Loss: 0.7864\n",
      "Epoch [56/100], Training Loss: 0.7837\n",
      "Epoch [57/100], Training Loss: 0.7820\n",
      "Epoch [58/100], Training Loss: 0.7792\n",
      "Epoch [59/100], Training Loss: 0.7782\n",
      "Epoch [60/100], Training Loss: 0.7746\n",
      "Epoch [61/100], Training Loss: 0.7744\n",
      "Epoch [62/100], Training Loss: 0.7719\n",
      "Epoch [63/100], Training Loss: 0.7698\n",
      "Epoch [64/100], Training Loss: 0.7685\n",
      "Epoch [65/100], Training Loss: 0.7659\n",
      "Epoch [66/100], Training Loss: 0.7642\n",
      "Epoch [67/100], Training Loss: 0.7612\n",
      "Epoch [68/100], Training Loss: 0.7612\n",
      "Epoch [69/100], Training Loss: 0.7596\n",
      "Epoch [70/100], Training Loss: 0.7571\n",
      "Epoch [71/100], Training Loss: 0.7574\n",
      "Epoch [72/100], Training Loss: 0.7553\n",
      "Epoch [73/100], Training Loss: 0.7529\n",
      "Epoch [74/100], Training Loss: 0.7511\n",
      "Epoch [75/100], Training Loss: 0.7500\n",
      "Epoch [76/100], Training Loss: 0.7478\n",
      "Epoch [77/100], Training Loss: 0.7467\n",
      "Epoch [78/100], Training Loss: 0.7438\n",
      "Epoch [79/100], Training Loss: 0.7443\n",
      "Epoch [80/100], Training Loss: 0.7432\n",
      "Epoch [81/100], Training Loss: 0.7411\n",
      "Epoch [82/100], Training Loss: 0.7384\n",
      "Epoch [83/100], Training Loss: 0.7383\n",
      "Epoch [84/100], Training Loss: 0.7360\n",
      "Epoch [85/100], Training Loss: 0.7326\n",
      "Epoch [86/100], Training Loss: 0.7338\n",
      "Epoch [87/100], Training Loss: 0.7317\n",
      "Epoch [88/100], Training Loss: 0.7293\n",
      "Epoch [89/100], Training Loss: 0.7287\n",
      "Epoch [90/100], Training Loss: 0.7261\n",
      "Epoch [91/100], Training Loss: 0.7261\n",
      "Epoch [92/100], Training Loss: 0.7237\n",
      "Epoch [93/100], Training Loss: 0.7236\n",
      "Epoch [94/100], Training Loss: 0.7231\n",
      "Epoch [95/100], Training Loss: 0.7209\n",
      "Epoch [96/100], Training Loss: 0.7197\n",
      "Epoch [97/100], Training Loss: 0.7188\n",
      "Epoch [98/100], Training Loss: 0.7175\n",
      "Epoch [99/100], Training Loss: 0.7186\n",
      "Epoch [100/100], Training Loss: 0.7150\n",
      "Validation Loss: 0.8482, Validation Accuracy: 0.6900\n",
      "Epoch [1/100], Training Loss: 1.0785\n",
      "Epoch [2/100], Training Loss: 1.0369\n",
      "Epoch [3/100], Training Loss: 1.0118\n",
      "Epoch [4/100], Training Loss: 0.9959\n",
      "Epoch [5/100], Training Loss: 0.9832\n",
      "Epoch [6/100], Training Loss: 0.9721\n",
      "Epoch [7/100], Training Loss: 0.9627\n",
      "Epoch [8/100], Training Loss: 0.9539\n",
      "Epoch [9/100], Training Loss: 0.9462\n",
      "Epoch [10/100], Training Loss: 0.9382\n",
      "Epoch [11/100], Training Loss: 0.9320\n",
      "Epoch [12/100], Training Loss: 0.9268\n",
      "Epoch [13/100], Training Loss: 0.9207\n",
      "Epoch [14/100], Training Loss: 0.9163\n",
      "Epoch [15/100], Training Loss: 0.9117\n",
      "Epoch [16/100], Training Loss: 0.9073\n",
      "Epoch [17/100], Training Loss: 0.9031\n",
      "Epoch [18/100], Training Loss: 0.8984\n",
      "Epoch [19/100], Training Loss: 0.8955\n",
      "Epoch [20/100], Training Loss: 0.8908\n",
      "Epoch [21/100], Training Loss: 0.8877\n",
      "Epoch [22/100], Training Loss: 0.8850\n",
      "Epoch [23/100], Training Loss: 0.8813\n",
      "Epoch [24/100], Training Loss: 0.8782\n",
      "Epoch [25/100], Training Loss: 0.8758\n",
      "Epoch [26/100], Training Loss: 0.8715\n",
      "Epoch [27/100], Training Loss: 0.8696\n",
      "Epoch [28/100], Training Loss: 0.8670\n",
      "Epoch [29/100], Training Loss: 0.8641\n",
      "Epoch [30/100], Training Loss: 0.8614\n",
      "Epoch [31/100], Training Loss: 0.8588\n",
      "Epoch [32/100], Training Loss: 0.8565\n",
      "Epoch [33/100], Training Loss: 0.8538\n",
      "Epoch [34/100], Training Loss: 0.8518\n",
      "Epoch [35/100], Training Loss: 0.8503\n",
      "Epoch [36/100], Training Loss: 0.8470\n",
      "Epoch [37/100], Training Loss: 0.8446\n",
      "Epoch [38/100], Training Loss: 0.8429\n",
      "Epoch [39/100], Training Loss: 0.8404\n",
      "Epoch [40/100], Training Loss: 0.8379\n",
      "Epoch [41/100], Training Loss: 0.8368\n",
      "Epoch [42/100], Training Loss: 0.8349\n",
      "Epoch [43/100], Training Loss: 0.8334\n",
      "Epoch [44/100], Training Loss: 0.8310\n",
      "Epoch [45/100], Training Loss: 0.8302\n",
      "Epoch [46/100], Training Loss: 0.8290\n",
      "Epoch [47/100], Training Loss: 0.8265\n",
      "Epoch [48/100], Training Loss: 0.8247\n",
      "Epoch [49/100], Training Loss: 0.8233\n",
      "Epoch [50/100], Training Loss: 0.8218\n",
      "Epoch [51/100], Training Loss: 0.8212\n",
      "Epoch [52/100], Training Loss: 0.8177\n",
      "Epoch [53/100], Training Loss: 0.8158\n",
      "Epoch [54/100], Training Loss: 0.8166\n",
      "Epoch [55/100], Training Loss: 0.8144\n",
      "Epoch [56/100], Training Loss: 0.8128\n",
      "Epoch [57/100], Training Loss: 0.8110\n",
      "Epoch [58/100], Training Loss: 0.8102\n",
      "Epoch [59/100], Training Loss: 0.8084\n",
      "Epoch [60/100], Training Loss: 0.8076\n",
      "Epoch [61/100], Training Loss: 0.8062\n",
      "Epoch [62/100], Training Loss: 0.8042\n",
      "Epoch [63/100], Training Loss: 0.8028\n",
      "Epoch [64/100], Training Loss: 0.8010\n",
      "Epoch [65/100], Training Loss: 0.8000\n",
      "Epoch [66/100], Training Loss: 0.7980\n",
      "Epoch [67/100], Training Loss: 0.7967\n",
      "Epoch [68/100], Training Loss: 0.7966\n",
      "Epoch [69/100], Training Loss: 0.7958\n",
      "Epoch [70/100], Training Loss: 0.7925\n",
      "Epoch [71/100], Training Loss: 0.7923\n",
      "Epoch [72/100], Training Loss: 0.7905\n",
      "Epoch [73/100], Training Loss: 0.7890\n",
      "Epoch [74/100], Training Loss: 0.7878\n",
      "Epoch [75/100], Training Loss: 0.7868\n",
      "Epoch [76/100], Training Loss: 0.7842\n",
      "Epoch [77/100], Training Loss: 0.7823\n",
      "Epoch [78/100], Training Loss: 0.7819\n",
      "Epoch [79/100], Training Loss: 0.7813\n",
      "Epoch [80/100], Training Loss: 0.7796\n",
      "Epoch [81/100], Training Loss: 0.7777\n",
      "Epoch [82/100], Training Loss: 0.7766\n",
      "Epoch [83/100], Training Loss: 0.7751\n",
      "Epoch [84/100], Training Loss: 0.7740\n",
      "Epoch [85/100], Training Loss: 0.7722\n",
      "Epoch [86/100], Training Loss: 0.7728\n",
      "Epoch [87/100], Training Loss: 0.7703\n",
      "Epoch [88/100], Training Loss: 0.7686\n",
      "Epoch [89/100], Training Loss: 0.7698\n",
      "Epoch [90/100], Training Loss: 0.7668\n",
      "Epoch [91/100], Training Loss: 0.7654\n",
      "Epoch [92/100], Training Loss: 0.7636\n",
      "Epoch [93/100], Training Loss: 0.7628\n",
      "Epoch [94/100], Training Loss: 0.7613\n",
      "Epoch [95/100], Training Loss: 0.7610\n",
      "Epoch [96/100], Training Loss: 0.7592\n",
      "Epoch [97/100], Training Loss: 0.7566\n",
      "Epoch [98/100], Training Loss: 0.7563\n",
      "Epoch [99/100], Training Loss: 0.7555\n",
      "Epoch [100/100], Training Loss: 0.7536\n",
      "Validation Loss: 0.8340, Validation Accuracy: 0.7157\n",
      "  Average Loss: 0.8411, Average Accuracy: 0.7029 for parameters {'learning_rate': 0.001, 'hidden_layers': [(512, 256), (256, 128), (128, 64), (64, 32), (32, 16)]}\n",
      "Epoch [1/100], Training Loss: 1.0915\n",
      "Epoch [2/100], Training Loss: 1.0576\n",
      "Epoch [3/100], Training Loss: 1.0288\n",
      "Epoch [4/100], Training Loss: 1.0105\n",
      "Epoch [5/100], Training Loss: 0.9979\n",
      "Epoch [6/100], Training Loss: 0.9869\n",
      "Epoch [7/100], Training Loss: 0.9790\n",
      "Epoch [8/100], Training Loss: 0.9715\n",
      "Epoch [9/100], Training Loss: 0.9658\n",
      "Epoch [10/100], Training Loss: 0.9596\n",
      "Epoch [11/100], Training Loss: 0.9540\n",
      "Epoch [12/100], Training Loss: 0.9504\n",
      "Epoch [13/100], Training Loss: 0.9460\n",
      "Epoch [14/100], Training Loss: 0.9419\n",
      "Epoch [15/100], Training Loss: 0.9388\n",
      "Epoch [16/100], Training Loss: 0.9343\n",
      "Epoch [17/100], Training Loss: 0.9323\n",
      "Epoch [18/100], Training Loss: 0.9288\n",
      "Epoch [19/100], Training Loss: 0.9258\n",
      "Epoch [20/100], Training Loss: 0.9232\n",
      "Epoch [21/100], Training Loss: 0.9202\n",
      "Epoch [22/100], Training Loss: 0.9180\n",
      "Epoch [23/100], Training Loss: 0.9148\n",
      "Epoch [24/100], Training Loss: 0.9131\n",
      "Epoch [25/100], Training Loss: 0.9105\n",
      "Epoch [26/100], Training Loss: 0.9080\n",
      "Epoch [27/100], Training Loss: 0.9056\n",
      "Epoch [28/100], Training Loss: 0.9040\n",
      "Epoch [29/100], Training Loss: 0.9020\n",
      "Epoch [30/100], Training Loss: 0.8993\n",
      "Epoch [31/100], Training Loss: 0.8973\n",
      "Epoch [32/100], Training Loss: 0.8956\n",
      "Epoch [33/100], Training Loss: 0.8937\n",
      "Epoch [34/100], Training Loss: 0.8925\n",
      "Epoch [35/100], Training Loss: 0.8904\n",
      "Epoch [36/100], Training Loss: 0.8883\n",
      "Epoch [37/100], Training Loss: 0.8866\n",
      "Epoch [38/100], Training Loss: 0.8846\n",
      "Epoch [39/100], Training Loss: 0.8826\n",
      "Epoch [40/100], Training Loss: 0.8817\n",
      "Epoch [41/100], Training Loss: 0.8799\n",
      "Epoch [42/100], Training Loss: 0.8778\n",
      "Epoch [43/100], Training Loss: 0.8762\n",
      "Epoch [44/100], Training Loss: 0.8744\n",
      "Epoch [45/100], Training Loss: 0.8721\n",
      "Epoch [46/100], Training Loss: 0.8701\n",
      "Epoch [47/100], Training Loss: 0.8679\n",
      "Epoch [48/100], Training Loss: 0.8674\n",
      "Epoch [49/100], Training Loss: 0.8653\n",
      "Epoch [50/100], Training Loss: 0.8641\n",
      "Epoch [51/100], Training Loss: 0.8617\n",
      "Epoch [52/100], Training Loss: 0.8605\n",
      "Epoch [53/100], Training Loss: 0.8588\n",
      "Epoch [54/100], Training Loss: 0.8565\n",
      "Epoch [55/100], Training Loss: 0.8554\n",
      "Epoch [56/100], Training Loss: 0.8530\n",
      "Epoch [57/100], Training Loss: 0.8519\n",
      "Epoch [58/100], Training Loss: 0.8508\n",
      "Epoch [59/100], Training Loss: 0.8488\n",
      "Epoch [60/100], Training Loss: 0.8471\n",
      "Epoch [61/100], Training Loss: 0.8458\n",
      "Epoch [62/100], Training Loss: 0.8437\n",
      "Epoch [63/100], Training Loss: 0.8422\n",
      "Epoch [64/100], Training Loss: 0.8409\n",
      "Epoch [65/100], Training Loss: 0.8393\n",
      "Epoch [66/100], Training Loss: 0.8376\n",
      "Epoch [67/100], Training Loss: 0.8355\n",
      "Epoch [68/100], Training Loss: 0.8355\n",
      "Epoch [69/100], Training Loss: 0.8331\n",
      "Epoch [70/100], Training Loss: 0.8314\n",
      "Epoch [71/100], Training Loss: 0.8304\n",
      "Epoch [72/100], Training Loss: 0.8288\n",
      "Epoch [73/100], Training Loss: 0.8272\n",
      "Epoch [74/100], Training Loss: 0.8253\n",
      "Epoch [75/100], Training Loss: 0.8238\n",
      "Epoch [76/100], Training Loss: 0.8226\n",
      "Epoch [77/100], Training Loss: 0.8208\n",
      "Epoch [78/100], Training Loss: 0.8183\n",
      "Epoch [79/100], Training Loss: 0.8185\n",
      "Epoch [80/100], Training Loss: 0.8172\n",
      "Epoch [81/100], Training Loss: 0.8159\n",
      "Epoch [82/100], Training Loss: 0.8139\n",
      "Epoch [83/100], Training Loss: 0.8118\n",
      "Epoch [84/100], Training Loss: 0.8115\n",
      "Epoch [85/100], Training Loss: 0.8099\n",
      "Epoch [86/100], Training Loss: 0.8087\n",
      "Epoch [87/100], Training Loss: 0.8072\n",
      "Epoch [88/100], Training Loss: 0.8063\n",
      "Epoch [89/100], Training Loss: 0.8053\n",
      "Epoch [90/100], Training Loss: 0.8038\n",
      "Epoch [91/100], Training Loss: 0.8023\n",
      "Epoch [92/100], Training Loss: 0.8008\n",
      "Epoch [93/100], Training Loss: 0.8007\n",
      "Epoch [94/100], Training Loss: 0.7995\n",
      "Epoch [95/100], Training Loss: 0.7978\n",
      "Epoch [96/100], Training Loss: 0.7966\n",
      "Epoch [97/100], Training Loss: 0.7945\n",
      "Epoch [98/100], Training Loss: 0.7936\n",
      "Epoch [99/100], Training Loss: 0.7932\n",
      "Epoch [100/100], Training Loss: 0.7913\n",
      "Validation Loss: 0.8763, Validation Accuracy: 0.6700\n",
      "Epoch [1/100], Training Loss: 1.0887\n",
      "Epoch [2/100], Training Loss: 1.0647\n",
      "Epoch [3/100], Training Loss: 1.0442\n",
      "Epoch [4/100], Training Loss: 1.0287\n",
      "Epoch [5/100], Training Loss: 1.0159\n",
      "Epoch [6/100], Training Loss: 1.0061\n",
      "Epoch [7/100], Training Loss: 0.9984\n",
      "Epoch [8/100], Training Loss: 0.9913\n",
      "Epoch [9/100], Training Loss: 0.9852\n",
      "Epoch [10/100], Training Loss: 0.9789\n",
      "Epoch [11/100], Training Loss: 0.9734\n",
      "Epoch [12/100], Training Loss: 0.9689\n",
      "Epoch [13/100], Training Loss: 0.9635\n",
      "Epoch [14/100], Training Loss: 0.9592\n",
      "Epoch [15/100], Training Loss: 0.9549\n",
      "Epoch [16/100], Training Loss: 0.9510\n",
      "Epoch [17/100], Training Loss: 0.9467\n",
      "Epoch [18/100], Training Loss: 0.9429\n",
      "Epoch [19/100], Training Loss: 0.9395\n",
      "Epoch [20/100], Training Loss: 0.9351\n",
      "Epoch [21/100], Training Loss: 0.9324\n",
      "Epoch [22/100], Training Loss: 0.9303\n",
      "Epoch [23/100], Training Loss: 0.9262\n",
      "Epoch [24/100], Training Loss: 0.9234\n",
      "Epoch [25/100], Training Loss: 0.9212\n",
      "Epoch [26/100], Training Loss: 0.9177\n",
      "Epoch [27/100], Training Loss: 0.9160\n",
      "Epoch [28/100], Training Loss: 0.9132\n",
      "Epoch [29/100], Training Loss: 0.9109\n",
      "Epoch [30/100], Training Loss: 0.9078\n",
      "Epoch [31/100], Training Loss: 0.9057\n",
      "Epoch [32/100], Training Loss: 0.9036\n",
      "Epoch [33/100], Training Loss: 0.9015\n",
      "Epoch [34/100], Training Loss: 0.8995\n",
      "Epoch [35/100], Training Loss: 0.8981\n",
      "Epoch [36/100], Training Loss: 0.8954\n",
      "Epoch [37/100], Training Loss: 0.8935\n",
      "Epoch [38/100], Training Loss: 0.8919\n",
      "Epoch [39/100], Training Loss: 0.8897\n",
      "Epoch [40/100], Training Loss: 0.8877\n",
      "Epoch [41/100], Training Loss: 0.8863\n",
      "Epoch [42/100], Training Loss: 0.8847\n",
      "Epoch [43/100], Training Loss: 0.8830\n",
      "Epoch [44/100], Training Loss: 0.8810\n",
      "Epoch [45/100], Training Loss: 0.8800\n",
      "Epoch [46/100], Training Loss: 0.8784\n",
      "Epoch [47/100], Training Loss: 0.8764\n",
      "Epoch [48/100], Training Loss: 0.8746\n",
      "Epoch [49/100], Training Loss: 0.8734\n",
      "Epoch [50/100], Training Loss: 0.8722\n",
      "Epoch [51/100], Training Loss: 0.8707\n",
      "Epoch [52/100], Training Loss: 0.8682\n",
      "Epoch [53/100], Training Loss: 0.8673\n",
      "Epoch [54/100], Training Loss: 0.8662\n",
      "Epoch [55/100], Training Loss: 0.8646\n",
      "Epoch [56/100], Training Loss: 0.8637\n",
      "Epoch [57/100], Training Loss: 0.8615\n",
      "Epoch [58/100], Training Loss: 0.8609\n",
      "Epoch [59/100], Training Loss: 0.8595\n",
      "Epoch [60/100], Training Loss: 0.8579\n",
      "Epoch [61/100], Training Loss: 0.8565\n",
      "Epoch [62/100], Training Loss: 0.8551\n",
      "Epoch [63/100], Training Loss: 0.8543\n",
      "Epoch [64/100], Training Loss: 0.8530\n",
      "Epoch [65/100], Training Loss: 0.8521\n",
      "Epoch [66/100], Training Loss: 0.8498\n",
      "Epoch [67/100], Training Loss: 0.8489\n",
      "Epoch [68/100], Training Loss: 0.8487\n",
      "Epoch [69/100], Training Loss: 0.8477\n",
      "Epoch [70/100], Training Loss: 0.8459\n",
      "Epoch [71/100], Training Loss: 0.8443\n",
      "Epoch [72/100], Training Loss: 0.8432\n",
      "Epoch [73/100], Training Loss: 0.8425\n",
      "Epoch [74/100], Training Loss: 0.8412\n",
      "Epoch [75/100], Training Loss: 0.8402\n",
      "Epoch [76/100], Training Loss: 0.8395\n",
      "Epoch [77/100], Training Loss: 0.8381\n",
      "Epoch [78/100], Training Loss: 0.8371\n",
      "Epoch [79/100], Training Loss: 0.8365\n",
      "Epoch [80/100], Training Loss: 0.8358\n",
      "Epoch [81/100], Training Loss: 0.8344\n",
      "Epoch [82/100], Training Loss: 0.8333\n",
      "Epoch [83/100], Training Loss: 0.8325\n",
      "Epoch [84/100], Training Loss: 0.8312\n",
      "Epoch [85/100], Training Loss: 0.8306\n",
      "Epoch [86/100], Training Loss: 0.8295\n",
      "Epoch [87/100], Training Loss: 0.8288\n",
      "Epoch [88/100], Training Loss: 0.8276\n",
      "Epoch [89/100], Training Loss: 0.8278\n",
      "Epoch [90/100], Training Loss: 0.8267\n",
      "Epoch [91/100], Training Loss: 0.8255\n",
      "Epoch [92/100], Training Loss: 0.8242\n",
      "Epoch [93/100], Training Loss: 0.8239\n",
      "Epoch [94/100], Training Loss: 0.8229\n",
      "Epoch [95/100], Training Loss: 0.8225\n",
      "Epoch [96/100], Training Loss: 0.8212\n",
      "Epoch [97/100], Training Loss: 0.8199\n",
      "Epoch [98/100], Training Loss: 0.8196\n",
      "Epoch [99/100], Training Loss: 0.8194\n",
      "Epoch [100/100], Training Loss: 0.8175\n",
      "Validation Loss: 0.8670, Validation Accuracy: 0.6793\n",
      "  Average Loss: 0.8716, Average Accuracy: 0.6746 for parameters {'learning_rate': 0.0005, 'hidden_layers': [(512, 256), (256, 128), (128, 64), (64, 32), (32, 16)]}\n"
     ]
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "581c795b098a277b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "46abc3dacd3448ec",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
